---
title: "Elasticsearch详解"
date: 2023-02-14
draft: false
tags: ["详解","分布式","Elasticsearch"]
slug: "java-elasticsearch"
---

## 概览
`Elasticsearch`简称为`ES`，它是一个开源的高扩展的分布式全文搜索引擎，是整个`ElasticStack`技术栈的核心。
它可以近乎实时的存储、检索数据，本身扩展性很好，可以扩展到上百台服务器，处理**PB级别**的数据。
> 1PB等于1024TB，这是一个非常大的数据量，通常用于描述大规模的数据存储需求，比如在云计算、大数据分析、数据中心等领域。

> `Elastic Stack`包括`Elasticsearch`、`Kibana`、`Beats`和`Logstash`，也称为 ELK。能够安全可靠地获取任何来源、任何格式的数据，然后实时地对数据进行搜索、分析和可视化。

`Elasticsearch`是面向文档型数据库，一条数据在这里就是一个文档。`Elasticsearch`里存储文档数据和关系型数据库MySQL存储数据的概念进行一个类比，如图：
![Elasticsearch详解-01](/iblog/posts/annex/images/essays/Elasticsearch详解-01.png)

`ES`里的`Index`可以看做一个库，而`Types`相当于表，`Documents`则相当于表的行。
这里`Types`的概念已经被逐渐弱化，`Elasticsearch 6.X`中，一个`index`下已经只能包含一个`type`，`Elasticsearch 7.X`中，`Type`的概念已经被删除了。

## 集群架构
一个运行中的`Elasticsearch`实例称为一个节点，而集群是由一个或者多个拥有相同`cluster.name`配置的节点组成的，它们共同承担数据和负载的压力。
当有节点加入集群中或者从集群中移除节点时，集群将会重新平均分布所有的数据。

当一个节点被选举成为主节点时，它将负责管理集群范围内的所有的变更，例如，增加、删除索引，或者增加、删除节点等。
而主节点并不需要涉及到文档级别的变更和搜索等操作，所以当集群只拥有一个主节点的情况下，即使流量的增加它也不会成为瓶颈。
任何节点都可以成为主节点。

作为用户，我们可以将请求发送到集群中的任何节点，包括主节点。每个节点都知道任意文档所处的位置，并且能够将我们的请求直接转发到存储我们所需文档的节点。
无论我们将请求发送到哪个节点，它都能负责从各个包含我们所需文档的节点收集回数据，并将最终结果返回给客户端。

### 搭建集群
1. 创建`elasticsearch-cluster`文件夹，在内部复制三个`elasticsearch`服务。

    ![Elasticsearch详解-02](/iblog/posts/annex/images/essays/Elasticsearch详解-02.png)

2. 修改节点配置`config/elasticsearch.yml`文件。
   - `node-1001`节点
      ```yaml
      #集群名称，节点之间要保持一致
      cluster.name: my-elasticsearch
      #节点名称，集群内要唯一
      node.name: node-1001
      node.master: true
      node.data: true
      #ip 地址
      network.host: localhost
      #http 端口
      http.port: 1001
      #tcp 监听端口
      transport.tcp.port: 9301
      #discovery.seed_hosts: ["localhost:9301", "localhost:9302","localhost:9303"]
      #discovery.zen.fd.ping_timeout: 1m
      #discovery.zen.fd.ping_retries: 5
      #集群内的可以被选为主节点的节点列表
      #cluster.initial_master_nodes: ["node-1", "node-2","node-3"]
      #跨域配置
      #action.destructive_requires_name: true
      http.cors.enabled: true
      http.cors.allow-origin: "*"
      ```
   - `node-1002`节点
     ```yaml
      #集群名称，节点之间要保持一致
      cluster.name: my-elasticsearch
      #节点名称，集群内要唯一
      node.name: node-1002
      node.master: true
      node.data: true
      #ip 地址
      network.host: localhost
      #http 端口
      http.port: 1002
      #tcp 监听端口
      transport.tcp.port: 9302
      discovery.seed_hosts: ["localhost:9301"]
      discovery.zen.fd.ping_timeout: 1m
      discovery.zen.fd.ping_retries: 5
      #集群内的可以被选为主节点的节点列表
      #cluster.initial_master_nodes: ["node-1", "node-2","node-3"]
      #跨域配置
      #action.destructive_requires_name: true
      http.cors.enabled: true
      http.cors.allow-origin: "*"
      ```
   - `node-1003`节点
     ```yaml
     #集群名称，节点之间要保持一致
     cluster.name: my-elasticsearch
     #节点名称，集群内要唯一
     node.name: node-1003
     node.master: true
     node.data: true
     #ip 地址
     network.host: localhost
     #http 端口
     http.port: 1003
     #tcp 监听端口
     transport.tcp.port: 9303
     #候选主节点的地址，在开启服务后可以被选为主节点
     discovery.seed_hosts: ["localhost:9301", "localhost:9302"]
     discovery.zen.fd.ping_timeout: 1m
     discovery.zen.fd.ping_retries: 5
     #集群内的可以被选为主节点的节点列表
     #cluster.initial_master_nodes: ["node-1", "node-2","node-3"]
     #跨域配置
     #action.destructive_requires_name: true
     http.cors.enabled: true
     http.cors.allow-origin: "*"
     ```
3. 启动集群，点击`bin\elasticsearch.bat`

    ![Elasticsearch详解-03](/iblog/posts/annex/images/essays/Elasticsearch详解-03.png)

如果启动不起来，可能原因是分配内存不足，需要修改`config\jvm.options`文件中的内存属性。
启动之后可使用[elasticsearch-head](https://github.com/mobz/elasticsearch-head),[ElasticHD](https://gitee.com/farmerx/ElasticHD)等`ES`可视化工具查看。

### 分布式架构原理
`ElasticSearch`设计的理念就是分布式搜索引擎，底层其实还是基于`lucene`的。
其核心思想就是在多台机器上启动多个`ES`进程实例，组成了一个`ES`集群。
`ES`分布式架构实际上就是对`index`进行拆分，将`index`拆分成多个分片，并将分片分别放到不同的`ES`上实现集群部署。

![Elasticsearch详解-05](/iblog/posts/annex/images/essays/Elasticsearch详解-05.png)

分片是其核心架构的一个重要组成部分，分片的设计使得`Elasticsearch`能够水平扩展，处理大规模数据和高负载查询。
分片是索引的子集，用于将数据分布到不同的节点上。每个分片都是一个完整的倒排索引，它独立处理存储和搜索任务。

它支持横向扩展。比如你数据量是3T，3个分片，每个分片就是1T的数据。若现在数据量增加到4T，怎么扩展？很简单，重新建一个有4个分片的索引将数据导进去；
还能提高性能。数据分布在多个分片，即多台服务器上，负载均衡分散了查询和索引操作的压力。查询请求会被分发到各个相关的分片上，提升了查询效率。

分片的数据实际上是有多个备份存在的，会存在一个主分片还有几个副本分片。如果主分片所在的节点发生故障，副本分片可以接管，确保数据不会丢失。
当写入数据的时候先写入主分片，然后并行将数据同步到副本分片上。当读数据的时候会获取到所有分片，然后负载均衡轮询读取。

当某个节点宕机了，还有其他分片副本保存在其他的机器上，从而实现了高可用。如果是非主节点宕机了，那么会由主节点，让那个宕机节点上的主分片的数据转移到其他机器上的副本数据。
接着你要是修复了那个宕机机器，重启了之后，主节点会控制将缺失的副本数据分配过去，同步后续修改的数据之类的，让集群恢复正常。如果是主节点宕机，那么会重新选举一个节点为主节点。

### 故障转移
在一个网络环境里，失败随时都可能发生，在某个分片/节点不知怎么的就处于离线状态，或者由于任何原因消失了。
这种情况下，有一个故障转移机制是非常有用并且是强烈推荐的。为此`Elasticsearch`允许创建分片的一份或多份拷贝，这些拷贝叫做复制分片。

当集群中只有一个节点在运行时，意味着会有一个单点故障问题。幸运的是，我们只需再启动一个节点即可防止数据丢失。
当你在同一台机器上启动了第二个节点时，只要它和第一个节点有同样的`cluster.name`配置，它就会自动发现集群并加入到其中。

集群中的节点是通过心跳机制周期性地检查其他节点的健康状态。当节点未响应时，集群会将其标记为不可用。
如果一个节点上的主分片发生故障，`Elasticsearch`会将其标记为“失效”。集群会自动选举一个副本分片作为新的主分片，来确保数据的可用性。
如果一个节点上的副本分片发生故障，集群会尝试从其他节点上的副本分片中恢复。集群会自动重新分配副本分片到其他健康节点，来保证数据的冗余备份。

当故障的节点恢复时，集群会自动将其重新加入，并重新分配分片。如果原来的主分片已被重新选举，恢复的节点会从其他节点接收数据，确保数据的一致性。
如果主分片发生故障并被替换，集群会将新的主分片的数据从副本分片恢复。`Elasticsearch`会将副本分片同步到恢复的节点，确保副本分片的最新状态。

集群中的节点使用心跳机制定期检查主节点的健康状态。如果主节点在一定时间内未响应，其他节点会认为主节点可能已故障。
当现有主节点故障时，集群会自动开始主节点选举过程。所有节点都可以作为候选主节点参与选举。
参与选举的节点会进行投票，选择一个候选节点成为新的主节点。当候选节点获得多数投票时，它将被选举为新的主节点。选举结果会被通知所有节点，新的主节点开始接管管理任务。

当一个节点掉线，如果该节点是`master`节点，则通过比较`node ID`，选择较小`ID`的节点为`master`。然后由`master`节点决定分片如何重新分配。同理，新加入节点也是由`master`决定如何分配分片。
但是`ES`在主节点的选举中会产生分歧，会产生多个主节点，从而使集群分裂，使得集群处于异常状态，这个现象叫做脑裂。
脑裂问题其实就是同一个集群的不同节点对于整个集群的状态有不同的理解，导致操作错乱，类似于精神分裂。
所以`ES`最好部署三个以上的节点，并且配置仲裁数大于一半节点，来防止`master`选举的脑裂问题。

### 分片控制
当写入一个文档的时候，文档会被存储到一个主分片中。`Elasticsearch`集群是如何知道一个文档应该存放到哪个分片中呢？

`Elasticsearch`集群路由计算公式：
```text
shard = hash(routing) % number_of_primary_shards
```
`routing`是一个可变值，默认是文档的`id`，也可以设置成一个自定义的值。`routing`通过`hash`函数生成一个数字，然后这个数字再除以`number_of_primary_shards`（主分片的数量）后得到余数。
这个分布在0到`number_of_primary_shards-1`之间的余数，就是我们所寻求的文档所在分片的位置。
这也就是创建索引的时候主分片的数量永远也不会改变的原因，如果数量变化了，那么所有之前路由的值都会无效，文档也再也找不到了。

### 写数据流程
![Elasticsearch详解-06](/iblog/posts/annex/images/essays/Elasticsearch详解-06.png)

1. 写请求接收：用户发起的写请求（如 `index`、`update`、`delete`）会发送到集群中的任意节点，称为协调节点。
2. 请求路由：协调节点将写请求路由到正确的主分片，负责处理写操作。
3. 主分片处理：主分片将文档写入内存中的缓冲区，并更新索引的内部数据结构。数据会周期性地从内存刷新到磁盘上的段文件中，完成持久化。
4. 同步副本：主分片将数据复制到所有副本分片，副本分片将数据写入内存缓冲区，并最终刷新到磁盘。
5. 写操作确认：主分片和副本分片完成写入和同步后，写请求被标记为成功。协调节点收集分片的确认信息，并将结果返回给客户端。
6. 持久化与刷新：定期刷新操作将内存中的数据刷新到磁盘，用户也可以强制刷新以将最新写操作纳入搜索中。
7. 数据一致性：写操作具有原子性，要么完全成功，要么完全失败。副本分片同步提供数据冗余备份，主分片发生故障时副本分片可以接管。

当客户端收到成功响应时，文档变更已在主分片和所有副本分片中完成。可以通过一些请求参数来影响这一过程，这可能会提升性能，但可能会降低数据安全性。
设置`consistency`参数值会影响写入操作。`consistency`参数的值可以设为：
- `one`：只要主分片状态正常，就允许执行写操作。
- `all`：必须要主分片和所有副本分片的状态正常才允许执行写操作。
- `quorum`：默认值为 `quorum`，即大多数的分片副本状态正常就允许执行写操作。如果没有足够的副本分片，`Elasticsearch`会等待。默认情况下，它最多等待1分钟，可以使用`timeout`参数控制。

### 读数据流程
1. 用户发起读请求（如 `search`、`get`），请求发送到集群中的任意节点，称为协调节点。
2. 协调节点将请求路由到主分片和副本分片。主分片处理请求，查询内存中的数据，必要时从磁盘读取数据，并将结果返回给协调节点。副本分片也接收请求，从内存或磁盘读取数据，返回结果给协调节点。
3. 协调节点从主分片和副本分片收集查询结果，进行合并和排序。
5. 协调节点最终返回的结果可能包括主分片的最新数据和副本分片的同步数据。这样，即使副本分片尚未完全更新，客户端也能收到最新的数据或正确的查询结果。

## 原理
[//]: # (写到了这里)
分片是`Elasticsearch`最小的工作单元。传统的数据库每个字段存储单个值，但这对全文检索并不够。文本字段中的每个单词需要被搜索，对数据库意味着需要单个字段有索引多值的能力。最好的支持是一个字段多个值需求的数据结构是倒排索引。

### 倒排索引
Elasticsearch 使用一种称为 倒排索引 的结构，它适用于快速的全文搜索。一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。 倒排索引（Inverted Index）也叫反向索引，有反向索引必有正向索引。通俗地来讲，正向索引是通过key找value，反向索引则是通过value找key。

倒排索引示例:
```text
| value    | key               |  
|----------|-------------------|   
| my name is zhangsan   | 1001 |  
```
   
```text               
| key     | value|
|---------|------|
| name    | 1001 |
| zhang   | 1001 |
| zhangsan| 1001 |
```
倒排索引搜索过程: 查询单词是否在词典中,如果不在搜索结束,如果在词典中需要查询单词在倒排列表中的指针,获取单词对应的文档ID,根据文档ID查询时哪一条数据

词条: 索引中最小的存储和查询单元
词典: 词条的集合;一般用hash表或B+tree存储
倒排表: 记录了出现过某个单词的所有文档的文档列表及单词在该文档中出现的位置信息,每条记录称为一个倒排项.根据倒排列表,即可获知哪些文档包含某个单词.

### 分析器
倒排索引总是和分词分不开的,中文分词和英文分词是不一样的,所以就需要分析器.  

分析器的主要功能是将一块文本分成适合于倒排索引的独立词条,分析器组成:
- 字符过滤器: 在分词前整理字符串,一个字符过滤器可以用来去掉 HTML,或者将 & 转化成 and;
- 分词器: 字符串被分词器分为单个的词条,一个简单的分词器遇到空格和标点的时候,可能会将文本拆分成词条;
- 词单元过滤器: 按顺序通过每个过滤器,这个过程可能会改变词条（例如，小写化Quick ），删除词条（例如， 像 a， and， the 等无用词），或者增加词条（例如，像jump和leap这种同义词）;

Elasticsearch附带了可以直接使用的预包装的分析器:
- 标准分析器: 默认使用的分析器。它是分析各种语言文本最常用的选择。它根据Unicode 联盟定义的单词边界划分文本。删除绝大部分标点;
- 简单分析器: 在任何不是字母的地方分隔文本，将词条小写;
- 空格分析器: 在空格的地方划分文本;
- 语言分析器: 考虑指定语言的特点,根据语法进行分词; 例如，英语分析器附带了一组英语无用词（常用单词，例如and或者the ,它们对相关性没有多少影响），它们会被删除;

常用中文分词器: [ik分词器](https://github.com/medcl/elasticsearch-analysis-ik/releases/tag/v7.8.0), 将解压后的后的文件夹放入 ES 根目录下的 plugins 目录下，重启 ES 即可使用.


### 文档搜索
早期的全文检索会为整个文档集合建立一个很大的倒排索引并将其写入到磁盘。 被写入的索引不可变化,一旦新的索引就绪，旧的就会被其替换.如果你需要让一个新的文档可被搜索，你需要重建整个索引。这要么对一个索引所能包含的数据量造成了很大的限制，要么对索引可被更新的频率造成了很大的限制。

如何在保留不变性的前提下实现倒排索引的更新?

用更多的索引。通过增加新的补充索引来反映新的修改，而不是直接重写整个倒排索引。每一个倒排索引都会被轮流查询到,从最早的开始查询完后再对结果进行合并。

当一个文档被删除时，它实际上只是在文件中被标记删除。一个被标记删除的文档仍然可以被查询匹配到，但它会在最终结果被返回前从结果集中过滤掉。 文档更新也是类似的操作方式:当一个文档被更新时，旧版本文档被标记删除，文档的新版本被索引到一个新的段中。可能两个版本的文档都会被一个查询匹配到，但被删除的那个旧版本文档在结果集返回前就已经被移除。
当一个查询被触发，所有已知的段按顺序被查询。词项统计会对所有段的结果进行聚合,此时会将标记删除的数据真正的删除.

### 近实时搜索
Elasticsearch 的主要功能就是搜索,但是Elasticsearch的搜索功能不是实时的,而是近实时的,主要原因在于ES搜索是分段搜索.

ES中的每一段就是一个倒排索引,最新的数据更新会体现在最新的段中,而最新的段落盘之后ES才能进行搜索,所以磁盘性能极大影响了ES软件的搜索.ES的主要作用就是快速准确的获取想要的数据,所以降低处理数据的延迟就显得尤为重要.

ES近实时搜索实现:
![Elasticsearch详解-04](/iblog/posts/annex/images/essays/Elasticsearch详解-04.png)

1. 一个文档被索引之后，就会被添加到内存缓冲区，并且追加到了 translog 事务日志中;(先写入索引中,再写入到日志中,目的防止数据丢失,类似数据库中的事务日志)
2. 将内存缓冲区中的分片刷新到磁盘中(refresh);此时缓冲区的数据可被搜索,当完全将数据写入磁盘会清空缓冲区中的数据;
3. 随着不断的刷写,磁盘中的文件会越来越多,此时需要文件段合并;当一个新的索引文件产生之后,文件的更新,删除便会体现出,此时在合并文件的时候便会真正的将数据删除;小的段被合并到大的段，然后这些大的段再被合并到更大的段;

## 优化
Elasticsearch 在数据量很大的情况下（数十亿级别）如何提高查询效率？

### 合理设置分片数
分片和副本的设计为 ES 提供了支持分布式和故障转移的特性，但并不意味着分片和副本是可以无限分配的。而且索引的分片完成分配后由于索引的路由机制，我们是不能重新修改分片数的.否则将无法找到对应的数据.

- 控制每个分片占用的硬盘容量不超过 ES 的最大 JVM 的堆空间设置，因此，如果索引的总容量在 500G 左右，那分片大小在 16 个左右即可;
- 考虑一下 node 数量，一般一个节点有时候就是一台物理机，如果分片数过多，大大超过了节点数，很可能会导致一个节点上存在多个分片，一旦该节点故障，即使保持了 1 个以上的副本，同样有可能会导致数据丢失，集群无法恢复。所以， 一般都设置分片数不超过节点数的 3 倍;
- 主分片，副本和节点最大数之间数量，我们分配的时候可以参考： 节点数<=主分片数 *（副本数+1）;


### [文件缓冲区](https://doocs.github.io/advanced-java/#/docs/high-concurrency/es-optimizing-query-performance)
往 ES 里写的数据，实际上都写到磁盘文件里去了，查询的时候，操作系统会将磁盘文件里的数据自动缓存到 filesystem cache 里面去;ES 的搜索引擎严重依赖于底层的 filesystem cache ，你如果给 filesystem cache 更多的内存，尽量让内存可以容纳所有的 idx segment file  索引数据文件，那么你搜索的时候就基本都是走内存的，性能会非常高。

> 案例: 某个公司 ES 节点有 3 台机器，每台机器看起来内存很多，64G，总内存就是 64 * 3 = 192G 。每台机器给 ES jvm heap 是 32G ，那么剩下来留给 filesystem cache 的就是每台机器才 32G ，总共集群里给 filesystem cache 的就是 32 * 3 = 96G 内存。而此时，整个磁盘上索引数据文件，在 3 台机器上一共占用了 1T 的磁盘容量，ES 数据量是 1T ，那么每台机器的数据量是 300G 。这样性能好吗？ filesystem cache 的内存才 100G，十分之一的数据可以放内存，其他的都在磁盘，然后你执行搜索操作，大部分操作都是走磁盘，性能肯定差。

归根结底，你要让 ES 性能要好，最佳的情况下，就是你的机器的内存，至少可以容纳你的总数据量的一半。

根据生产环境实践经验，最佳的情况下，是仅仅在 ES 中就存少量的数据，就是你要用来搜索的那些索引，如果内存留给 filesystem cache 的是 100G，那么你就将索引数据控制在 100G 以内，这样的话，你的数据几乎全部走内存来搜索，性能非常之高，一般可以在 1 秒以内。

比如说你现在有一行数据。 id,name,age .... 30 个字段。但是你现在搜索，只需要根据 id,name,age 三个字段来搜索。如果你傻乎乎往 ES 里写入一行数据所有的字段，就会导致说 90% 的数据是不用来搜索的，结果硬是占据了 ES 机器上的 filesystem cache 的空间，单条数据的数据量越大，就会导致 filesystem cahce 能缓存的数据就越少。其实，仅仅写入 ES 中要用来检索的少数几个字段就可以了，比如说就写入 ES id,name,age 三个字段，然后你可以把其他的字段数据存在 mysql/hbase 里，我们一般是建议用 ES + hbase 这么一个架构。

写入 ES 的数据最好小于等于，或者是略微大于 ES 的 filesystem cache 的内存容量。然后你从 ES 检索可能就花费 20ms，然后再根据 ES 返回的 id 去 hbase 里查询，查 20 条数据，可能也就耗费个 30ms，可能你原来那么玩儿，1T 数据都放 es，会每次查询都是 5~10s，现在可能性能就会很高，每次查询就是 50ms。


### [数据预热](https://doocs.github.io/advanced-java/#/docs/high-concurrency/es-optimizing-query-performance)

假如说，哪怕是你就按照上述的方案去做了，ES 集群中每个机器写入的数据量还是超过了 filesystem cache 一倍，比如说你写入一台机器 60G 数据，结果 filesystem cache 就 30G，还是有 30G 数据留在了磁盘上。

其实可以做数据预热。

举个例子，拿微博来说，你可以把一些大 V，平时看的人很多的数据，你自己提前后台搞个系统，每隔一会儿，自己的后台系统去搜索一下热数据，刷到 filesystem cache 里去，后面用户实际上来看这个热数据的时候，他们就是直接从内存里搜索了，很快。

或者是电商，你可以将平时查看最多的一些商品，比如说 iphone 8，热数据提前后台搞个程序，每隔 1 分钟自己主动访问一次，刷到 filesystem cache 里去。

对于那些你觉得比较热的、经常会有人访问的数据，最好做一个专门的缓存预热子系统，就是对热数据每隔一段时间，就提前访问一下，让数据进入 filesystem cache 里面去。这样下次别人访问的时候，性能一定会好很多。

### [冷热分离](https://doocs.github.io/advanced-java/#/docs/high-concurrency/es-optimizing-query-performance)
ES 可以做类似于 mysql 的水平拆分，就是说将大量的访问很少、频率很低的数据，单独写一个索引，然后将访问很频繁的热数据单独写一个索引。最好是将冷数据写入一个索引中，然后热数据写入另外一个索引中，这样可以确保热数据在被预热之后，尽量都让他们留在 filesystem os cache 里，别让冷数据给冲刷掉。

假设你有 6 台机器，2 个索引，一个放冷数据，一个放热数据，每个索引 3 个 shard。3 台机器放热数据 index，另外 3 台机器放冷数据 index。然后这样的话，你大量的时间是在访问热数据 index，热数据可能就占总数据量的 10%，此时数据量很少，几乎全都保留在 filesystem cache 里面了，就可以确保热数据的访问性能是很高的。但是对于冷数据而言，是在别的 index 里的，跟热数据 index 不在相同的机器上，大家互相之间都没什么联系了。如果有人访问冷数据，可能大量数据是在磁盘上的，此时性能差点，就 10% 的人去访问冷数据，90% 的人在访问热数据，也无所谓了。


### [document模型设计](https://doocs.github.io/advanced-java/#/docs/high-concurrency/es-optimizing-query-performance)
对于 MySQL，我们经常有一些复杂的关联查询。在 ES 里该怎么玩儿，ES 里面的复杂的关联查询尽量别用，一旦用了性能一般都不太好。

最好是先在 Java 系统里就完成关联，将关联好的数据直接写入 ES 中。搜索的时候，就不需要利用 ES 的搜索语法来完成 join 之类的关联搜索了。

document 模型设计是非常重要的，很多操作，不要在搜索的时候才想去执行各种复杂的乱七八糟的操作。ES 能支持的操作就那么多，不要考虑用 ES 做一些它不好操作的事情。如果真的有那种操作，尽量在 document 模型设计的时候，写入的时候就完成。另外对于一些太复杂的操作，比如 join/nested/parent-child 搜索都要尽量避免，性能都很差的。





